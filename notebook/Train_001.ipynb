{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "athletic-honey",
   "metadata": {},
   "source": [
    "## ライブラリインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dental-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "# from tensorflow.compat.v1.keras.applications import EfficientNetB0 \n",
    "# tf.compat.v1.keras.applications.efficientnet.EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subsequent-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_randvalue(value):\n",
    "    # Set a seed value\n",
    "    seed_value= value \n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "seed_value = 42\n",
    "set_randvalue(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-gallery",
   "metadata": {},
   "source": [
    "## CSVロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operational-raise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_100026.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_10003.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_100050.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image label\n",
       "0  img_100026.jpg     0\n",
       "1   img_10003.jpg     0\n",
       "2  img_100050.jpg     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.csvが元のラベルごとにディレクトリに保存されていたデータから作成したcsv\n",
    "df = pd.read_csv(\"../data/input/csvs/train.csv\")\n",
    "df[\"label\"] = df[\"label\"].astype(str)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-theorem",
   "metadata": {},
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "sorted-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0のモデルを作成\n",
    "def create_model(weight_flg=False):\n",
    "    weight = None\n",
    "    if weight_flg:\n",
    "        weight = \"../model/efficientnetb0_notop.h5\" # ImageNetで学習されたモデルをロード\n",
    "    # include_top=False; 全結合層なし\n",
    "    base_model = EfficientNetB0(weights=weight, include_top=False, pooling='avg', input_shape=(224,224,3))\n",
    "    x = Dense(512, activation='relu')(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "    model = Model(base_model.input, output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-moldova",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simple-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "img_size = 224\n",
    "batch_size = 16\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "catholic-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator作成\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20, # rotation range\n",
    "    width_shift_range=30/img_size, # 30 pixel\n",
    "    height_shift_range=30/img_size, # 30 pixel\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "drawn-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17939 validated image filenames belonging to 10 classes.\n",
      "Found 4485 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1121/1121 [==============================] - 173s 155ms/step - loss: 2.0103 - accuracy: 0.2334 - val_loss: 1.3334 - val_accuracy: 0.4449\n",
      "Epoch 2/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 1.2506 - accuracy: 0.5215 - val_loss: 0.7346 - val_accuracy: 0.7201\n",
      "Epoch 3/250\n",
      "1121/1121 [==============================] - 170s 152ms/step - loss: 0.7471 - accuracy: 0.7416 - val_loss: 0.3694 - val_accuracy: 0.8708\n",
      "Epoch 4/250\n",
      "1121/1121 [==============================] - 171s 152ms/step - loss: 0.5498 - accuracy: 0.8224 - val_loss: 0.3854 - val_accuracy: 0.8792\n",
      "Epoch 5/250\n",
      "1121/1121 [==============================] - 171s 153ms/step - loss: 0.5035 - accuracy: 0.8398 - val_loss: 0.4298 - val_accuracy: 0.8451\n",
      "Epoch 6/250\n",
      "1121/1121 [==============================] - 172s 153ms/step - loss: 0.3540 - accuracy: 0.8939 - val_loss: 0.1832 - val_accuracy: 0.9478\n",
      "Epoch 7/250\n",
      "1121/1121 [==============================] - 171s 153ms/step - loss: 0.3218 - accuracy: 0.9063 - val_loss: 0.1818 - val_accuracy: 0.9513\n",
      "Epoch 8/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2843 - accuracy: 0.9184 - val_loss: 0.1280 - val_accuracy: 0.9650\n",
      "Epoch 9/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2603 - accuracy: 0.9257 - val_loss: 0.1039 - val_accuracy: 0.9710\n",
      "Epoch 10/250\n",
      "1121/1121 [==============================] - 167s 149ms/step - loss: 0.2331 - accuracy: 0.9344 - val_loss: 0.1296 - val_accuracy: 0.9663\n",
      "Epoch 11/250\n",
      "1121/1121 [==============================] - 167s 149ms/step - loss: 0.2074 - accuracy: 0.9432 - val_loss: 0.1229 - val_accuracy: 0.9627\n",
      "Epoch 12/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.2026 - accuracy: 0.9455 - val_loss: 0.0936 - val_accuracy: 0.9725\n",
      "Epoch 13/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.1967 - accuracy: 0.9467 - val_loss: 0.1310 - val_accuracy: 0.9641\n",
      "Epoch 14/250\n",
      "1121/1121 [==============================] - 171s 153ms/step - loss: 0.1775 - accuracy: 0.9517 - val_loss: 0.2565 - val_accuracy: 0.9208\n",
      "Epoch 15/250\n",
      "1121/1121 [==============================] - 172s 153ms/step - loss: 0.1546 - accuracy: 0.9596 - val_loss: 0.0779 - val_accuracy: 0.9790\n",
      "Epoch 16/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1641 - accuracy: 0.9550 - val_loss: 0.1298 - val_accuracy: 0.9607\n",
      "Epoch 17/250\n",
      "1121/1121 [==============================] - 170s 152ms/step - loss: 0.1505 - accuracy: 0.9608 - val_loss: 0.0741 - val_accuracy: 0.9833\n",
      "Epoch 18/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1684 - accuracy: 0.9560 - val_loss: 0.0995 - val_accuracy: 0.9748\n",
      "Epoch 19/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1410 - accuracy: 0.9634 - val_loss: 0.0971 - val_accuracy: 0.9748\n",
      "Epoch 20/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1403 - accuracy: 0.9645 - val_loss: 0.0759 - val_accuracy: 0.9801\n",
      "Epoch 00020: early stopping\n",
      "Found 17939 validated image filenames belonging to 10 classes.\n",
      "Found 4485 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1121/1121 [==============================] - 174s 155ms/step - loss: 2.2243 - accuracy: 0.1556 - val_loss: 3.3953 - val_accuracy: 0.1897\n",
      "Epoch 2/250\n",
      "1121/1121 [==============================] - 173s 154ms/step - loss: 1.5680 - accuracy: 0.3728 - val_loss: 1.1022 - val_accuracy: 0.5462\n",
      "Epoch 3/250\n",
      "1121/1121 [==============================] - 170s 152ms/step - loss: 1.0732 - accuracy: 0.5909 - val_loss: 0.6639 - val_accuracy: 0.7446\n",
      "Epoch 4/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.7123 - accuracy: 0.7488 - val_loss: 0.4650 - val_accuracy: 0.8478\n",
      "Epoch 5/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.5258 - accuracy: 0.8248 - val_loss: 0.3433 - val_accuracy: 0.8888\n",
      "Epoch 6/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.4267 - accuracy: 0.8627 - val_loss: 0.2760 - val_accuracy: 0.9100\n",
      "Epoch 7/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.3869 - accuracy: 0.8807 - val_loss: 0.3186 - val_accuracy: 0.9069\n",
      "Epoch 8/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.3106 - accuracy: 0.9075 - val_loss: 0.1658 - val_accuracy: 0.9583\n",
      "Epoch 9/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.2723 - accuracy: 0.9217 - val_loss: 0.2353 - val_accuracy: 0.9406\n",
      "Epoch 10/250\n",
      "1121/1121 [==============================] - 170s 152ms/step - loss: 0.2359 - accuracy: 0.9321 - val_loss: 0.2640 - val_accuracy: 0.9254\n",
      "Epoch 11/250\n",
      "1121/1121 [==============================] - 171s 153ms/step - loss: 0.2249 - accuracy: 0.9354 - val_loss: 0.1357 - val_accuracy: 0.9652\n",
      "Epoch 12/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.2359 - accuracy: 0.9331 - val_loss: 0.2722 - val_accuracy: 0.9163\n",
      "Epoch 13/250\n",
      "1121/1121 [==============================] - 172s 153ms/step - loss: 0.1892 - accuracy: 0.9445 - val_loss: 0.1135 - val_accuracy: 0.9683\n",
      "Epoch 14/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1863 - accuracy: 0.9481 - val_loss: 0.1051 - val_accuracy: 0.9746\n",
      "Epoch 15/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1684 - accuracy: 0.9551 - val_loss: 0.1542 - val_accuracy: 0.9578\n",
      "Epoch 16/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1634 - accuracy: 0.9569 - val_loss: 0.0837 - val_accuracy: 0.9772\n",
      "Epoch 17/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1986 - accuracy: 0.9449 - val_loss: 0.0713 - val_accuracy: 0.9842\n",
      "Epoch 18/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1512 - accuracy: 0.9605 - val_loss: 0.2216 - val_accuracy: 0.9402\n",
      "Epoch 19/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1625 - accuracy: 0.9552 - val_loss: 0.0806 - val_accuracy: 0.9808\n",
      "Epoch 20/250\n",
      "1121/1121 [==============================] - 170s 151ms/step - loss: 0.1544 - accuracy: 0.9577 - val_loss: 0.0763 - val_accuracy: 0.9821\n",
      "Epoch 00020: early stopping\n",
      "Found 17939 validated image filenames belonging to 10 classes.\n",
      "Found 4485 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1121/1121 [==============================] - 174s 155ms/step - loss: 2.2808 - accuracy: 0.1238 - val_loss: 2.0204 - val_accuracy: 0.1962\n",
      "Epoch 2/250\n",
      "1121/1121 [==============================] - 173s 154ms/step - loss: 1.8018 - accuracy: 0.2948 - val_loss: 1.2981 - val_accuracy: 0.4518\n",
      "Epoch 3/250\n",
      "1121/1121 [==============================] - 172s 154ms/step - loss: 1.3803 - accuracy: 0.4563 - val_loss: 1.8797 - val_accuracy: 0.4062\n",
      "Epoch 4/250\n",
      "1121/1121 [==============================] - 170s 152ms/step - loss: 1.1683 - accuracy: 0.5644 - val_loss: 2.4020 - val_accuracy: 0.3938\n",
      "Epoch 5/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.8414 - accuracy: 0.7008 - val_loss: 0.5064 - val_accuracy: 0.8179\n",
      "Epoch 6/250\n",
      "1121/1121 [==============================] - 245s 218ms/step - loss: 0.7012 - accuracy: 0.7566 - val_loss: 0.4946 - val_accuracy: 0.8103\n",
      "Epoch 7/250\n",
      "1121/1121 [==============================] - 221s 197ms/step - loss: 0.5788 - accuracy: 0.7992 - val_loss: 0.3328 - val_accuracy: 0.8804\n",
      "Epoch 8/250\n",
      "1121/1121 [==============================] - 167s 149ms/step - loss: 0.4954 - accuracy: 0.8380 - val_loss: 0.2517 - val_accuracy: 0.9185\n",
      "Epoch 9/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.4439 - accuracy: 0.8604 - val_loss: 0.2347 - val_accuracy: 0.9246\n",
      "Epoch 10/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.3892 - accuracy: 0.8784 - val_loss: 0.2471 - val_accuracy: 0.9283\n",
      "Epoch 11/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.3194 - accuracy: 0.9036 - val_loss: 0.2194 - val_accuracy: 0.9362\n",
      "Epoch 12/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.3076 - accuracy: 0.9112 - val_loss: 0.1362 - val_accuracy: 0.9603\n",
      "Epoch 13/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2778 - accuracy: 0.9181 - val_loss: 0.1782 - val_accuracy: 0.9525\n",
      "Epoch 14/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2952 - accuracy: 0.9151 - val_loss: 0.1679 - val_accuracy: 0.9491\n",
      "Epoch 15/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2416 - accuracy: 0.9315 - val_loss: 0.1452 - val_accuracy: 0.9614\n",
      "Epoch 00015: early stopping\n",
      "Found 17939 validated image filenames belonging to 10 classes.\n",
      "Found 4485 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1121/1121 [==============================] - 172s 153ms/step - loss: 2.1417 - accuracy: 0.1790 - val_loss: 2.0718 - val_accuracy: 0.2562\n",
      "Epoch 2/250\n",
      "1121/1121 [==============================] - 171s 153ms/step - loss: 1.5567 - accuracy: 0.3537 - val_loss: 1.2610 - val_accuracy: 0.4692\n",
      "Epoch 3/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 1.1542 - accuracy: 0.5382 - val_loss: 0.9029 - val_accuracy: 0.6991\n",
      "Epoch 4/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.7684 - accuracy: 0.7315 - val_loss: 0.5553 - val_accuracy: 0.7886\n",
      "Epoch 5/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.5254 - accuracy: 0.8210 - val_loss: 0.3283 - val_accuracy: 0.8882\n",
      "Epoch 6/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.4041 - accuracy: 0.8721 - val_loss: 0.3121 - val_accuracy: 0.8933\n",
      "Epoch 7/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.3538 - accuracy: 0.8939 - val_loss: 0.2874 - val_accuracy: 0.9217\n",
      "Epoch 8/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.3022 - accuracy: 0.9142 - val_loss: 0.2598 - val_accuracy: 0.9299\n",
      "Epoch 9/250\n",
      "1121/1121 [==============================] - 170s 152ms/step - loss: 0.2729 - accuracy: 0.9232 - val_loss: 0.1589 - val_accuracy: 0.9560\n",
      "Epoch 10/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2475 - accuracy: 0.9309 - val_loss: 0.1282 - val_accuracy: 0.9656\n",
      "Epoch 11/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.2204 - accuracy: 0.9380 - val_loss: 0.1224 - val_accuracy: 0.9701\n",
      "Epoch 12/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2313 - accuracy: 0.9366 - val_loss: 0.3654 - val_accuracy: 0.8989\n",
      "Epoch 13/250\n",
      "1121/1121 [==============================] - 171s 152ms/step - loss: 0.1869 - accuracy: 0.9489 - val_loss: 0.1817 - val_accuracy: 0.9576\n",
      "Epoch 14/250\n",
      "1121/1121 [==============================] - 172s 154ms/step - loss: 0.1872 - accuracy: 0.9494 - val_loss: 0.1112 - val_accuracy: 0.9643\n",
      "Epoch 15/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1787 - accuracy: 0.9517 - val_loss: 0.1394 - val_accuracy: 0.9654\n",
      "Epoch 16/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1706 - accuracy: 0.9546 - val_loss: 0.0879 - val_accuracy: 0.9790\n",
      "Epoch 17/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1589 - accuracy: 0.9580 - val_loss: 0.1778 - val_accuracy: 0.9538\n",
      "Epoch 18/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1554 - accuracy: 0.9580 - val_loss: 0.1235 - val_accuracy: 0.9701\n",
      "Epoch 19/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1364 - accuracy: 0.9643 - val_loss: 0.1129 - val_accuracy: 0.9766\n",
      "Epoch 00019: early stopping\n",
      "Found 17940 validated image filenames belonging to 10 classes.\n",
      "Found 4484 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1121/1121 [==============================] - 174s 155ms/step - loss: 2.3024 - accuracy: 0.1264 - val_loss: 2.1441 - val_accuracy: 0.1942\n",
      "Epoch 2/250\n",
      "1121/1121 [==============================] - 171s 153ms/step - loss: 1.6913 - accuracy: 0.3365 - val_loss: 1.1925 - val_accuracy: 0.5033\n",
      "Epoch 3/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 1.1464 - accuracy: 0.5493 - val_loss: 0.7873 - val_accuracy: 0.6777\n",
      "Epoch 4/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.8411 - accuracy: 0.6882 - val_loss: 0.4353 - val_accuracy: 0.8393\n",
      "Epoch 5/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.5459 - accuracy: 0.8165 - val_loss: 0.3635 - val_accuracy: 0.8754\n",
      "Epoch 6/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.4256 - accuracy: 0.8688 - val_loss: 0.2080 - val_accuracy: 0.9368\n",
      "Epoch 7/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.3648 - accuracy: 0.8945 - val_loss: 0.1947 - val_accuracy: 0.9379\n",
      "Epoch 8/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.3146 - accuracy: 0.9124 - val_loss: 0.1507 - val_accuracy: 0.9571\n",
      "Epoch 9/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2687 - accuracy: 0.9251 - val_loss: 0.1757 - val_accuracy: 0.9518\n",
      "Epoch 10/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.2696 - accuracy: 0.9264 - val_loss: 0.2216 - val_accuracy: 0.9350\n",
      "Epoch 11/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2300 - accuracy: 0.9381 - val_loss: 0.1286 - val_accuracy: 0.9638\n",
      "Epoch 12/250\n",
      "1121/1121 [==============================] - 169s 150ms/step - loss: 0.1973 - accuracy: 0.9458 - val_loss: 0.1227 - val_accuracy: 0.9721\n",
      "Epoch 13/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1989 - accuracy: 0.9461 - val_loss: 0.4991 - val_accuracy: 0.8810\n",
      "Epoch 14/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.2026 - accuracy: 0.9473 - val_loss: 0.0935 - val_accuracy: 0.9708\n",
      "Epoch 15/250\n",
      "1121/1121 [==============================] - 168s 150ms/step - loss: 0.1745 - accuracy: 0.9528 - val_loss: 0.0678 - val_accuracy: 0.9846\n",
      "Epoch 16/250\n",
      "1121/1121 [==============================] - 169s 151ms/step - loss: 0.1580 - accuracy: 0.9587 - val_loss: 0.6330 - val_accuracy: 0.8029\n",
      "Epoch 17/250\n",
      "1121/1121 [==============================] - 171s 152ms/step - loss: 0.1565 - accuracy: 0.9585 - val_loss: 0.1036 - val_accuracy: 0.9708\n",
      "Epoch 18/250\n",
      "1121/1121 [==============================] - 171s 152ms/step - loss: 0.1451 - accuracy: 0.9603 - val_loss: 0.0986 - val_accuracy: 0.9770\n",
      "Epoch 00018: early stopping\n",
      "Wall time: 4h 23min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 学習と評価データでのラベルの分布数を一定に保ち学習\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "splitter = skf.split(df[\"image\"],df[\"label\"])\n",
    "for i, (train_ids, valid_ids) in enumerate(splitter, 1):\n",
    "    # データ生成\n",
    "    train, valid = df.iloc[train_ids], df.iloc[valid_ids]\n",
    "    train_datagenerator = train_datagen.flow_from_dataframe(\n",
    "        train,\n",
    "        directory='../data/input/imgs/train/imgs/',\n",
    "        x_col='image',\n",
    "        y_col='label',\n",
    "        target_size=(img_size, img_size),\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        seed=seed_value\n",
    "    )\n",
    "\n",
    "    valid_datagenerator = valid_datagen.flow_from_dataframe(\n",
    "        valid,\n",
    "        directory='../data/input/imgs/train/imgs/',\n",
    "        x_col='image',\n",
    "        y_col='label',\n",
    "        target_size=(img_size, img_size),\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        seed=seed_value\n",
    "    )\n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    # 早期終了\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "    \n",
    "    # 早期終了するのでval_lossが小さいモデルを保存\n",
    "    model_path = '../model/' +  'Train001_' + \"fold\" + str(i) + \"_best_model.h5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                    filepath=model_path,\n",
    "                    monitor='val_loss',\n",
    "                    save_best_only=True,\n",
    "                    period=1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_datagenerator,\n",
    "        steps_per_epoch=int(len(train)//batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_datagenerator,\n",
    "        validation_steps=int(len(valid)//batch_size),\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-suite",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "normal-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model()\n",
    "model_path = \"../Train001_fold\" + str(0) + \"_best_model.h5\"\n",
    "model2.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "falling-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/input/csvs/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wrong-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    submit,\n",
    "    directory='../data/input/imgs/test/',\n",
    "    x_col='img',\n",
    "    y_col='c0', # ダミー変数\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode=None,\n",
    "    batch_size=1,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-nurse",
   "metadata": {},
   "source": [
    "#### 5モデルのうち1モデルで推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "voluntary-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_1 = create_model()\n",
    "model_1.load_weights('../model/Train001_fold1_best_model.h5')\n",
    "pred_1 = model_1.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "blessed-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "advisory-murder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "general-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "diverse-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1_df['img'] = submit['img'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "general-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1_df[labels] = pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "breeding-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1_df.to_csv('../data/output/Train001_fold1_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-signal",
   "metadata": {},
   "source": [
    "#### 5モデルのうち4モデルで推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "broad-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = create_model()\n",
    "model_.load_weights('../model/Train001_fold2_best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "unable-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values\n",
    "\n",
    "def inference(fold_num):\n",
    "    model = create_model()\n",
    "    weight_path = '../model/Train001_fold' + str(fold_num) + '_best_model.h5'\n",
    "    output_path = '../data/output/Train001_fold' + str(fold_num) + '_sub.csv'\n",
    "    model.load_weights(weight_path)\n",
    "    \n",
    "    pred = model.predict(test_generator, verbose=1)\n",
    "    pred_df = pd.DataFrame(columns=columns)\n",
    "    pred_df['img'] = submit['img']\n",
    "    pred_df[labels] = pred\n",
    "    pred_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fantastic-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79726/79726 [==============================] - 491s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/79726 [26:13<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79726/79726 [==============================] - 482s 6ms/step\n",
      "79726/79726 [==============================] - 487s 6ms/step\n",
      "79726/79726 [==============================] - 490s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "inference(2)\n",
    "inference(3)\n",
    "inference(4)\n",
    "inference(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-upgrade",
   "metadata": {},
   "source": [
    "#### アンサンブル：単純平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "standard-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/input/csvs/sample_submission.csv')\n",
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values\n",
    "\n",
    "ensemble = 0\n",
    "for i in range(1,6):\n",
    "    path = \"../data/output/Train001_fold\" + str(i) +\"_sub.csv\"\n",
    "    ensemble += pd.read_csv(path).values[:,1:] / 5 # fold数で割る\n",
    "\n",
    "ensemble_df = pd.DataFrame(columns=columns)\n",
    "ensemble_df['img'] = submit['img']\n",
    "ensemble_df[labels] = ensemble\n",
    "\n",
    "ensemble_df.to_csv(\"../data/output/Train001_ensemble_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-syntax",
   "metadata": {},
   "source": [
    "#### アンサンブル：加重平均\n",
    "\n",
    "fold数ごとのPublic scoreを元に加重平均  \n",
    "  \n",
    " fold: Public score  \n",
    " fold1: 0.60428  \n",
    " fold2: 0.57041  \n",
    " fold3: 0.65702  \n",
    " fold4: 0.50968  \n",
    " fold5: 0.69935  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "wrapped-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1_inverse = 1/0.60428  \n",
    "fold2_inverse = 1/0.57041  \n",
    "fold3_inverse = 1/0.65702  \n",
    "fold4_inverse = 1/0.50968  \n",
    "fold5_inverse = 1/0.69935  \n",
    "\n",
    "fold_inverse_sum = fold1_inverse + fold2_inverse + fold3_inverse + fold4_inverse + fold5_inverse\n",
    "\n",
    "fold1_weight = fold1_inverse/(fold_inverse_sum)\n",
    "fold2_weight = fold2_inverse/(fold_inverse_sum)\n",
    "fold3_weight = fold3_inverse/(fold_inverse_sum)\n",
    "fold4_weight = fold4_inverse/(fold_inverse_sum)\n",
    "fold5_weight = fold5_inverse/(fold_inverse_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "celtic-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/input/csvs/sample_submission.csv')\n",
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values\n",
    "\n",
    "weighted_ensemble = 0\n",
    "for i in range(1,6):\n",
    "    path = \"../data/output/Train001_fold\" + str(i) +\"_sub.csv\"\n",
    "    weight = 1\n",
    "    if i == 1: weight = fold1_weight\n",
    "    elif i == 2: weight = fold2_weight\n",
    "    elif i == 3: weight = fold3_weight\n",
    "    elif i == 4: weight = fold4_weight\n",
    "    elif i == 5: weight = fold5_weight\n",
    "        \n",
    "    weighted_ensemble += pd.read_csv(path).values[:,1:] * weight # foldの重みを掛ける\n",
    "\n",
    "ensemble_df = pd.DataFrame(columns=columns)\n",
    "ensemble_df['img'] = submit['img']\n",
    "ensemble_df[labels] = weighted_ensemble\n",
    "\n",
    "ensemble_df.to_csv(\"../data/output/Train001_weighted_ensemble_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-librarian",
   "metadata": {},
   "source": [
    "#### 推論時のデータ拡張: TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "attended-retention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "tta_num = 3\n",
    "\n",
    "tta_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20, # rotation range\n",
    "    width_shift_range=30/224, # 30 pixel\n",
    "    height_shift_range=30/224, # 30 pixel\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "tta_generator = tta_datagen.flow_from_dataframe(\n",
    "    submit,\n",
    "    directory='../data/input/imgs/test/',\n",
    "    x_col='img',\n",
    "    y_col='c0', # ダミー変数\n",
    "    target_size=(224, 224),\n",
    "    class_mode=None,\n",
    "    batch_size=1,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "handmade-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "submit = pd.read_csv('../data/input/csvs/sample_submission.csv')\n",
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values\n",
    "\n",
    "# len(test_generator)\n",
    "\n",
    "def inference(fold_num):\n",
    "    print(f\"Fold {fold_num} inference\")\n",
    "    model = create_model()\n",
    "    weight_path = '../model/Train001_fold' + str(fold_num) + '_best_model.h5'\n",
    "    output_path = '../data/output/Train001_TTA_fold' + str(fold_num) + '_sub.csv'\n",
    "    model.load_weights(weight_path)\n",
    "    \n",
    "    # Test time augumentation: 3 times\n",
    "    preds = 0\n",
    "    for i in range(tta_num):\n",
    "        print(f\"TTA {i+1}\")\n",
    "        preds += model.predict(tta_generator, verbose=1) / tta_num \n",
    "\n",
    "#     pred = model.predict(test_generator, verbose=1)\n",
    "    pred_df = pd.DataFrame(columns=columns)\n",
    "    pred_df['img'] = submit['img']\n",
    "    pred_df[labels] = preds\n",
    "    pred_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "rotary-welsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 inference\n",
      "TTA 1\n",
      "79726/79726 [==============================] - 2565s 32ms/step\n",
      "TTA 2\n",
      "79726/79726 [==============================] - 2072s 26ms/step\n",
      "TTA 3\n",
      "79726/79726 [==============================] - 2182s 27ms/step\n",
      "Fold 2 inference\n",
      "TTA 1\n",
      "79726/79726 [==============================] - 2118s 27ms/step\n",
      "TTA 2\n",
      "79726/79726 [==============================] - 2177s 27ms/step\n",
      "TTA 3\n",
      "79726/79726 [==============================] - 2289s 29ms/step\n",
      "Fold 3 inference\n",
      "TTA 1\n",
      "79726/79726 [==============================] - 2287s 29ms/step\n",
      "TTA 2\n",
      "79726/79726 [==============================] - 2255s 28ms/step\n",
      "TTA 3\n",
      "79726/79726 [==============================] - 2289s 29ms/step\n",
      "Fold 4 inference\n",
      "TTA 1\n",
      "79726/79726 [==============================] - 2244s 28ms/step\n",
      "TTA 2\n",
      "79726/79726 [==============================] - 1913s 24ms/step\n",
      "TTA 3\n",
      "79726/79726 [==============================] - 1939s 24ms/step\n",
      "Fold 5 inference\n",
      "TTA 1\n",
      "79726/79726 [==============================] - 2211s 28ms/step\n",
      "TTA 2\n",
      "79726/79726 [==============================] - 2168s 27ms/step\n",
      "TTA 3\n",
      "79726/79726 [==============================] - 2181s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "inference(1)\n",
    "inference(2)\n",
    "inference(3)\n",
    "inference(4)\n",
    "inference(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-ensemble",
   "metadata": {},
   "source": [
    "#### TTA アンサンブル：単純平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fresh-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/input/csvs/sample_submission.csv')\n",
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values\n",
    "\n",
    "ensemble = 0\n",
    "for i in range(1,6):\n",
    "    path = \"../data/output/Train001_TTA_fold\" + str(i) +\"_sub.csv\"\n",
    "    ensemble += pd.read_csv(path).values[:,1:] / 5 # fold数で割る\n",
    "\n",
    "ensemble_df = pd.DataFrame(columns=columns)\n",
    "ensemble_df['img'] = submit['img']\n",
    "ensemble_df[labels] = ensemble\n",
    "\n",
    "ensemble_df.to_csv(\"../data/output/Train001_TTA_ensemble_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-gateway",
   "metadata": {},
   "source": [
    "#### TTA アンサンブル：加重平均\n",
    "\n",
    "fold数ごとのPublic scoreを元に加重平均  \n",
    "  \n",
    " fold: Public score  \n",
    " fold1: 0.56271  \n",
    " fold2: 0.51018  \n",
    " fold3: 0.58452  \n",
    " fold4: 0.53186  \n",
    " fold5: 0.62222  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "important-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1_weight = 0.2\n",
    "fold2_weight = 0.25\n",
    "fold3_weight = 0.2\n",
    "fold4_weight = 0.25\n",
    "fold5_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "owned-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/input/csvs/sample_submission.csv')\n",
    "columns = submit.columns.values\n",
    "labels = submit.columns[1:].values\n",
    "\n",
    "weighted_ensemble = 0\n",
    "for i in range(1,6):\n",
    "    path = \"../data/output/Train001_TTA_fold\" + str(i) +\"_sub.csv\"\n",
    "    weight = 1\n",
    "    if i == 1: weight = fold1_weight\n",
    "    elif i == 2: weight = fold2_weight\n",
    "    elif i == 3: weight = fold3_weight\n",
    "    elif i == 4: weight = fold4_weight\n",
    "    elif i == 5: weight = fold5_weight\n",
    "        \n",
    "    weighted_ensemble += pd.read_csv(path).values[:,1:] * weight # foldの重みを掛ける\n",
    "\n",
    "ensemble_df = pd.DataFrame(columns=columns)\n",
    "ensemble_df['img'] = submit['img']\n",
    "ensemble_df[labels] = weighted_ensemble\n",
    "\n",
    "ensemble_df.to_csv(\"../data/output/Train001_TTA_weighted_ensemble_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-native",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
